---
title: "Class 4 notes and code"
output: html_document
---



$\\$



```{r, eval = FALSE}

download.file("https://yale.box.com/shared/static/1syezdzked6v2l3aqqpffmifww0468ez.rda", "misc_data.Rda")

download.file("https://yale.box.com/shared/static/p1akv464cut78w859pqqn15ebroxa2wm.rda", "players_born_1901_1950.Rda")


# install.packages("latex2exp")


```




```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```




$\\$




## Part 1: Questions about homework 1? 


$\\$



#### Part 1.1: Adding names to side-by-side boxplots 

$\\$


We can add names below each boxplot in a side-by-side boxplot by setting the `names` argument to a vector of strings.


```{r}

load("misc_data.Rda")

# add names to the side-by-side boxplots
boxplot(x1, x2, x3, x4,
        ylab = "Values")


```





#### Part 1.2: Removing data to make nicer plots


$\\$



Below is a solution to problem 3.3 of homework 1, where I tried to make a better looking pie chart by eliminating all states that had fewer than 200 baseball players born in them.



$\\$





```{r}

# download.file("https://yale.box.com/shared/static/p1akv464cut78w859pqqn15ebroxa2wm.rda", "players_born_1901_1950.Rda")

load("players_born_1901_1950.Rda")

birth_place_counts <- table(players_born_1901_1950$birthState)

inds_counts_greater_200 <- birth_place_counts > 200

birth_place_counts_greater_200 <- birth_place_counts[inds_counts_greater_200]

colors = c("red", "orange", "yellow", "green", "blue", "cyan", "skyblue", "purple", "magenta")

pie(birth_place_counts_greater_200, col = colors)

```


**Question:** What is problematic about this pie chart below? 



$\\$





Let's group all states with fewer than 200 players born in them into a single category called "Other". 

```{r}


# create a boolean vector of indices TRUE for for table entries with 200 of fewer counts


# sum together all the table entries with fewer than 200 counts


# add a category called "other" that has the total counts from states < 200 born in them


# create the pie chart (use the cex = .8 argument to make the text smaller)


```




$\\$




## Part 2: Review


$\\$


#### Part 2.1: For loops


$\\$


As discussed last class, for loops are useful when you want to repeat a piece of code many times under similar conditions. This is particularly useful for accumulating results in a vector. 

Let's accumulate the values 2^i for i in the range 1 to 100, and then plot them using the plot() function. 


```{r}

# create a loop to accumulate results for 2^i
the_results <- NULL



# plot the results


```




$\\$



## Part 3: Probability functions and generating random numbers in R


$\\$


#### Part 3.1: Probability density functions 



$\\$


Probability density functions can be used to model random events. All **probability density functions**, *f(x)*, have these properties:

1. The function are always non-negative
2. The area under the function integrates (sums) to 1

Which of the following are probability density functions? 


![](https://raw.githubusercontent.com/emeyers/SDS230_F19/master/class_images/class_04/which_are_prob_densities.png)




$\\$


For continuous (quantitative) data, we use density function f(x) to find the probability (e.g., model of the long run frequency) that a random outcome X is between two values *a* and *b* using:


$P(a < X < b) = \int_{a}^{b}f(x)dx$


![](https://raw.githubusercontent.com/emeyers/SDS230_F19/master/class_images/class_04/area_pdf.gif)




$\\$



#### Part 3.2: Probability density functions in R


$\\$


If we want to plot the true probability density function for the standard uniform distribution U(0, 1) we can use the dunif() function. All density function in base R start with d. 

```{r}


# the x-value domain for the density function f(x)
x <- seq(-.2, 1.2, by = .001)  
# the density function values y values


# plot the probability density function 


```



**Question:** Can you create density plots for the normal and exponential distributions? 



$\\$




#### Part 3.3: Generating random numbers in R

R has built in functions to generate data from different distributions. All these functions start with the letter r. 

We can also set the random number generator seed to always get the same sequence of random numbers.

Let's get a sample of n = 100 random points from the uniform distribution using runif()

```{r}

# set the seed to a specific number to always get the same sequence of random numbers

# generate n = 100 points from U(0, 1) using runif() function 

# plot a histogram of these random numbers


```



$\\$



We can also calculate statistics for the central tendency such as the mean $\bar{x}$ and median $m$ and statistics of spread such as the standard deviation $s$ and the interquartile range. Use the mean(), median(), sd(), and IQR() functions to calculate these statistics on the sample of data we generated above. Also use the round() function to only show 3 decimal places of precision. 


```{r}

# Print the mean, median, sd, and IQR
# Also use the round() function to round to 3 decimal places




```


Note: closing assignment in parentheses causes the output to be printed. You should always print the output of your calculations to show your answers.  





$\\$




## Part 4: The law of large numbers


$\\$


The *law of large numbers* states that as the sample size *n* grows the sample average approaches the expected value (i.e., the parameter $\mu$). We can write this as $n \rightarrow \infty$  $\bar{x} \rightarrow \mu$. 

Let's use R to examine how the precision of the sample statistic for the mean ($\bar{x}$) changes as the sample size *n* increases. 


```{r}

# Using a for loop to calculate statistics from random samples of size n as n increases
xbars <- NULL



library(latex2exp)

# plot the results (note \\ is needed for the lates2exp package)


```



Let's examine this using data sample of sizes n is powers of 2 to see what happens with larger sample sizes. 


```{r}

# use a for loop to calculate means for normal data where n is powers of 2
xbars <- NULL



# plot the results



# add a red horizontal line at the true parameter value




```



This is a nice theoretical property that our statistic converges to our parameter value (i.e., that our statistic is consistent), but we live in the *real world* where we have finite data. What is of real interest is if we have a finite sample size *n* what does the *distribution of statistics* we get look like, i.e., what does the **sampling distribtuion** look like? 






$\\$







## Part 5: Sampling distributions


#### Part 5.1

A distribution of statistics is called a **sampling distribution** 

Can you generate and plot a sampling distribution for:
 * sample means $\bar{x}$'s 
 * sample size n = 100
 * for data that come from uniform distribution


Note the shape of the *sampling distribution* can be quite different from the shape of the data distribution (which is uniform here).


```{r}

sampling_dist <- NULL

# create a sampling distribution of the mean using data from a uniform distribution



# plot a histogram of the sampling distribution of these means 



```




$\\$




#### Part 5.2: The standard error (SE)

The deviation of a sampling distribution is called the standard error (SE). Can you calculate the standard error for the sampling distribution you created above? 

```{r}



```




$\\$




#### Part 5.3: Is the data distribution normal?  Quantile-quantile plots

We can check if emperical data seems to come from a particular distribution using quantile-quantile plots (qqplots). qqplots plot the sorted data values in your sample as a function of the theoretical quantile values (at evenly spaced probability areas).

Below is an illustration of theoretical quantile values for 10 data points. If we have 10 data points in our sample, then to create a qqplot comparing our data to a standard normal distribution, we would plot our data as a function of these theoretical quantile values. If the plot falls along a diagonal line, this indicates our data comes from the standard normal distribution 

Also see [this explanation](https://www.statisticshowto.datasciencecentral.com/q-q-plots/)

![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2015/08/qq-plot.png)



Let's create a qqplot to compare the 1,000 points we created in our sampling distribution of the mean above to a normal distribution. 



```{r}

# create an sequaence of values between 0 and 1 at even spaces
prob_area_vals <- seq(0, 1, length.out = length(sampling_dist))

# get the quantiles from these values using the qnorm function

# create the q-q plot


```





$\\$




We can also use the qqnorm() function to do this more easily when comparing data to the normal distribution.


```{r}


```





Our result show that the data in our sampling distribution is pretty normal. 

Why is this? 



$\\$





#### Part 5.4:  The central limit theorm

The [central limit theorm (CTL)](https://en.wikipedia.org/wiki/Central_limit_theorem) establishes that (in most situations) when independent random variables are added their (normalized) sum converges to a normal distribution.

Put another way, if we definte the average random (i.i.d) sample {$X_1$, $X_2$, ..., $X_n$} of size *n* as: 

$S_{n}:={\frac{X_{1}+\cdots +X_{n}}{n}}$

then the CTL tells us that:

$\sqrt{n}(S_{n} - \mu)$ $\xrightarrow {d} N(0,\sigma^{2})$


You will explore this more through simulations on homework 2. 





$\\$





#### Part 5.5: Sampling and sampling distributions from a data set


We generate samples from an actual data set we have using the sample() function.  

Let's start by just generate a single sample of size n = 100 from the OkCupid users' heights and calculating the mean of this sample.


```{r}

library(okcupiddata)

# get the heights for the OkCupid data

# get one random sample of heights from 100 people

# get the mean of this sample


```



$\\$



We can then create a sampling distribution from the OkCupid users' data set by repeating this many times in a for loop.



```{r}


# repeat the process 1,000 times
sampling_dist <- NULL 



# plot a histogram of this sampling distribution 


```





$\\$





Above we know that the 100 values we get are samples OkCupid data set which has 59,946 OkCupid users from San Francisco. Samples always come from a larger population so...

**Question:** What is the population here? 

 a) The data set (data frame) of 59,946 cases?
 b) All OkCupid users? 
 c) All US citizens?
 d) All adults in the world?

 
It is important to define what the larger population is when doing statistical inference since you are really only answering questions about this population. 




$\\$




## Part 6: Confidence intervals


$\\$


Confidence intervals are ranges of values that capture a parameter a fix proportion of time. Let's explore this via wits and wagers.



